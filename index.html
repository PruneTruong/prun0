<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/preview.jpg">
  <title>Prune Truong- Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Prune Truong</name>
          <!--<br>
          ruthfong at robots dot ox dot ac dot uk-->
        </p>
        <p>
          I am a fourth year PhD student at <a href="https://ethz.ch/en.html">ETH Zurich</a> in the <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>.
            My supervisors are <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof. Luc Van Gool</a> and <a href="https://martin-danelljan.github.io/">Dr. Martin Danelljan</a>.
            My main research interests are Computer Vision and its applications, especially in the tasks of image matching, 3D reconstruction, pose estimation and novel-view rendering.
        <p>
          During my PhD I was a research intern at Google working with <a href="https://scholar.google.com/citations?user=TFsE4BIAAAAJ&hl=en">Federico Tombari</a>, and Microsoft <a href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-zurich/">Mixed Reality & AI Labs</a>, hosted by <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ&hl=en">Marc Pollofeys</a>.
            I am also one of the recipient of the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2022">Apple AI/ML PhD Fellowship</a> of 2022.
        <p>
            Prior to that, I obtained a Master’s degree in Mechanical Engineering with honors at ETH Zurich. I also conducted an internship at <a href="https://www.retinai.com/">RetinAI</a> focused on computer vision applied to medical images.
        </p>

        <!-- <p>
          I have also spent some time at <a href="https://adsc.illinois.edu">ADSC</a> and <a href="https://www.a-star.edu.sg/">A*STAR</a> in Singapore.
        </p> -->
        <span style="color:#ff0000;">I will finish my PhD in fall 2023, and I am actively looking for Zurich-based (or partly remote) research scientist positions or exciting startup opportunities.</span>
        <p align=center>
          <a href="mailto:prune.truong@vision.ee.ethz.ch">Email</a> &nbsp|&nbsp
          <a href="files/CV_prune_truong.pdf">CV</a> &nbsp|&nbsp
          <!--<a href="files/ruth_fong_bio.txt">Biography</a> &nbsp/&nbsp-->
<!--           <a href="https://scholar.google.com/citations?user=39cUD3gAAAAJ">Google Scholar</a> &nbsp/&nbsp -->
          <a href="https://github.com/PruneTruong">GitHub</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=v8VCED0AAAAJ&hl=fr">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/prune-truong-00922a12b/"> LinkedIn</a>
          &nbsp|&nbsp
          <a href="https://twitter.com/prunetruong">Twitter</a>
        </p>
        </td>
        <td width="33%">
          <img src="media/profile.jpg" width="250" alt="headshot">
          <!--<div class="one">
          <div class="two" id="headshot_image"><img src="media/color_headshot.png" width="250" alt="headshot"></div>
          <img src="media/bw_headshot.png" width="250" alt="headshot">
          </div>
          <script type="text/javascript">
          function headshot_start() {
          document.getElementById('headshot_image').style.opacity = "1";
          }
          function headshot_stop() {
          document.getElementById('headshot_image').style.opacity = "0";
          }
          filters_stop()
          </script>-->
        </td>
        <!--<td width="33%">
        <img src="media/bw_headshot.png" width="250">
        <img src="media/color_headshot.png" width="250">
        </td>-->
      </tr>
      <!-- <td width="10%"><a href="adsc.illinois.edu"><img src="media/adsc_logo.png" width="100"></a></td>
      <td width="10%"><a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="100"></a></td>
      <td width="10%"><a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="100"></a></td>
      <td width="10%"><a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="50"></a></td> -->
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://about.facebook.com/realitylabs/"><img src="media/microsoft_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/retinai_logo.jpeg" width="120"></a>
        </td>

        
        <!-- <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="80"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="100"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="70"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="40"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="60"></a></td>
        </td>	 -->
      </tr>
      </table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
              <li><strong>04/2023</strong> Our paper <a href="https://prunetruong.com/sparf.github.io/">SPARF</a> got accepted to <strong>CVPR 2023</strong> as a <strong>HIGHLIGHT</strong>!</li>
              <br>
              <li><strong>01/2023</strong> We will be in <strong>WACV</strong> to present our paper <a href="https://arxiv.org/abs/2207.06825">Refign</a>, on semantic segmentation in adverse conditions!</li>
              <br>
              <li><strong>07/2022</strong> I started internship at Google hosted by <a href="https://scholar.google.com/citations?user=TFsE4BIAAAAJ&hl=en">Federico Tombari</a></li>
              <br>
              <li><strong>04/2022</strong> I started an internship at Microsoft <a href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-zurich/">Mixed Reality & AI Labs</a>, hosted by <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ&hl=en">Marc Pollofeys</a></li>
            <br>
              <li><strong>04/2022</strong> I got awarded an <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2022">Apple AI/ML PhD Fellowship</a>!</li>
              <br>
              <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;">
              <br>
                
                <li><strong>03/2022</strong> Our paper <a href="https://arxiv.org/abs/2203.04279">Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences</a> will be presented at <strong>CVPR 2022</strong>!</li>
                <br>
                <li><strong>09/2021</strong> Take a look at our TPAMI journal extension <a href="https://arxiv.org/abs/2109.13912">PDC-Net+</a>! We show cool applications in image-based localization and image retrieval.</li>
                <br>
                <li><strong>08/2021</strong> Our paper <a href="https://arxiv.org/abs/2104.03308">Warp Consistency for Unsupervised Learning of Dense Correspondences</a> was accepted at <strong>ICCV 2021</strong> as an <strong>ORAL</strong>!</li>
            <br>
                <li><strong>07/2021</strong> We released <a href="https://github.com/PruneTruong/DenseMatching">DenseMatching</a>, a general PyTorch dense matching library.</li>
           <br>
            <li><strong>06/2021</strong> I gave a talk about our ORAL CVPR 2021 paper <a href="https://arxiv.org/abs/2101.01710">PDC-Net</a> at the <a href="https://image-matching-workshop.github.io/">Image Matching: Local Features & Beyond workshop</a>.</li>
            <br>
                <li><strong>04/2021</strong> Check out our new preprint <a href="https://arxiv.org/abs/2104.03308">Warp Consistency for Unsupervised Learning of Dense Correspondences</a>!</li>
                <br>
                <li><strong>03/2021</strong> Our paper <a href="https://arxiv.org/abs/2101.01710">Learning Accurate Dense Correspondences and When to Trust Them</a> (PDC-Net) was accepted at <strong>CVPR 2021</strong> as an <strong>ORAL</strong>!</li>
                <br>
                <li><strong>09/2020</strong> Our paper <a href="https://arxiv.org/abs/2009.07823">GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network</a> was accepted at <strong>NeurIPS 2020</strong>!</li>
            <br>
                <li><strong>08/2020</strong> We just released a new pre-print <a href="https://arxiv.org/abs/2009.07823">GOCor</a>. Code will be released soon here!</li>
            </div></div>
            </ul>
        </td>
      </tr>


      <tr>
        <td width="100%" valign="middle">
          <heading>My research</heading>
        </td>
      </tr>
      </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">  
      <td width="25%">
        <img src='media/thumnail_sparf.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
      		<a href="https://prunetruong.com/sparf.github.io/">
            <papertitle>SPARF: Neural Radiance Fields from Sparse and Noisy Poses</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Marie-Julie Rakotosaona,
          Fabian Manhardt,
          Federico Tombari
      <br>
          <em><strong>CVPR</strong> 2023</em> - <strong><span style="color:#ff0000;">Highlight</span> (top 2.5%)</strong>
   
      <br>
        <a href="bib/sparf2023.txt">citation</a> |
        <a href="https://arxiv.org/abs/2211.11738">paper</a> |
        <a href="https://prunetruong.com/sparf.github.io/">project page</a> |
        <a href="https://www.youtube.com/embed/_s3_p2Brd_8">video (8 min)</a> |
        <a href="https://www.youtube.com/embed/ARMKrcJlULE">teaser video</a> |
        <a href="https://drive.google.com/file/d/1uLsUZbOEf9DqfxY7xEU2urlJjGdfwKkI/view">poster</a> |
        <a href="https://github.com/google-research/sparf">code</a>
        <p></p>
        We propose SPARF, a joint pose-NeRF refinement approach, applicable to extreme scenarios with only 2/3
        input views and noisy camera poses. SPARF is the only method producing realistic novel-view synthesis
        from as few as 2 input images with noisy poses.
        <p></p>
      </td>
    </tr>
  </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_refign.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions</papertitle></a>
      <br>
          David Bruggemann,
          Christos Sakaridis,
          <strong>Prune Truong</strong>,
          Luc Van Gool
      <br>
          <em><strong>WACV</strong> 2023</em>

      <br>
        <a href="bib/refign2023.txt">citation</a> |
        <a href="https://arxiv.org/abs/2207.06825">paper</a> |
        <a href="https://github.com/brdav/refign">code</a>
        <p></p>
        We propose Refign, a generic extension to self-training-based domain adaptation methods for semantic segmentation.
        <p></p>
      </td>
    </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_pdcnet+.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
      		<a href="https://prunetruong.com/pdcnet+">
            <papertitle>PDC-Net+: Enhanced Probabilistic Dense Correspondence Network</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Radu Timofte,
          Luc Van Gool
      <br>
          <em><strong>TPAMI</strong> 2023</em>

      <br>
        <a href="bib/pdcnet+2023.txt">citation</a> |
        <a href="https://arxiv.org/abs/2109.13912">paper</a> |
        <a href="https://github.com/PruneTruong/DenseMatching">code</a>
        <p></p>
        We propose an approach for estimating dense correspondences between two views along with a confidence map. We extend
        PDC-Net with new applications to image-based localization, 3D reconstructions and texture-transfer.
        <p></p>
      </td>
    </tr>
      </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_pwarpc.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences</papertitle>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Fisher Yu,
          Luc Van Gool
      <br>
          <em><strong>CVPR</strong> 2022</em>

      <br>
        <a href="bib/pwarpc2022.txt">citation</a> |
        <a href="https://arxiv.org/abs/2203.04279">paper</a> |
        <a href="https://www.youtube.com/watch?v=I2KtnvI8xZU">teaser video</a> |
        <a href="https://drive.google.com/file/d/1lP5E3BNqdKJL1q-YsQ-C7rOwkcb5S63W/view?usp=sharing">poster</a> |
        <a href="https://github.com/PruneTruong/DenseMatching">code</a>
        <p></p>
        We propose a weakly-supervised training strategy for learning semantic correspondences. We introduce a
        weakly-supervised training objective applied to probabilistic mapping as well as an approach to model
        and identify occluded and unmatchable regions.
        <p></p>
      </td>
    </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumbnail_warpc.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>Warp Consistency for Unsupervised Learning of Dense Correspondences</papertitle>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Fisher Yu,
          Luc Van Gool
      <br>
          <em><strong>ICCV</strong> 2021</em> - <strong><span style="color:#ff0000;">Oral</span> (top 3.0%)</strong>

      <br>
        <a href="bib/warpc2021.txt">citation</a> |
        <a href="https://arxiv.org/abs/2104.03308">paper</a> |
        <a href="https://www.youtube.com/watch?v=IsMotj7-peA">teaser video</a> |
        <a href="https://drive.google.com/file/d/1PCXkjxvVsjHAbYzsBtgKWLO1uE6oGP6p/view?usp=sharing">poster</a> |
        <a href="https://github.com/PruneTruong/DenseMatching">code</a>
        <p></p>
        We propose an unsupervised training objective for learning to regress the dense correspondences relating a pair
        of images. Our loss leverages real image pairs without invoking the photometric consistency assumption.
        Unlike previous approaches, it is capable of handling large appearance and view-point changes.
        <p></p>
      </td>
    </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_pdcnet.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
      		<a href="https://prunetruong.com/pdcnet">
            <papertitle>Learning Accurate Dense Correspondences and When to Trust Them</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Luc Van Gool,
          Radu Timofte
      <br>
          <em><strong>CVPR</strong> 2021</em> - <strong><span style="color:#ff0000;">Oral</span> (top 4.0%)</strong>

      <br>
        <a href="bib/pdcnet2021.txt">citation</a> |
        <a href="https://arxiv.org/abs/2101.01710">paper</a> |
        <a href="https://youtu.be/bX0rEaSf88o">teaser video</a> |
        <a href="https://drive.google.com/file/d/18ya__AdEIgZyix8dXuRpJ15tdrpbMUsB/view?usp=sharing">poster</a> |
        <a href="https://drive.google.com/file/d/1zUQmpmVp6WSa_psuI3KFvKVrNyJE-beG/view?usp=sharing">slides</a> |
        <a href="https://github.com/PruneTruong/DenseMatching">code</a>
        <p></p>
        We develop a flexible probabilistic approach that jointly learns the dense correspondence prediction and
        its uncertainty. We parametrize the predictive distribution as a constrained mixture model and
        develop an architecture and training strategy tailored for robust and generalizable uncertainty
        prediction in the context of self-supervised training.
        <p></p>
      </td>
    </tr>
      </table>



      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_GOCor.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Luc Van Gool,
          Radu Timofte
      <br>
          <em><strong>NeurIPS</strong> 2020</em>

      <br>
        <a href="bib/gocor2020.txt">citation</a> |
        <a href="https://arxiv.org/abs/2009.07823">paper</a> |
        <a href="https://www.youtube.com/watch?v=V22MyFChBCs&feature=youtu.be">teaser video</a> |
        <a href="https://www.youtube.com/watch?v=V-kPP9buPYU">CV Talks video</a> |
        <a href="https://github.com/PruneTruong/GOCor">code</a>
        <p></p>
        We propose GOCOr, a fully differentiable dense matching module, acting as a direct replacement to the
        feature correlation layer. The correspondence volume generated by our module is the result of an
        internal optimization procedure that explicitly accounts for - and suppressed - similar regions in the scene.
        <p></p>
      </td>
    </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_glunet.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>GLU-Net: Global-Local Universal Network for dense flow and correspondences</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Martin Danelljan,
          Luc Van Gool,
          Radu Timofte
      <br>
          <em><strong>CVPR</strong> 2020</em> - <strong><span style="color:#ff0000;">Oral</span> (top 5.7%)</strong>

      <br>
        <a href="bib/glunet2020.txt">citation</a> |
        <a href="https://arxiv.org/abs/1912.05524">paper</a> |
        <a href="https://www.youtube.com/watch?v=s5OUdkM9QLo">teaser video</a> |
        <a href=https://drive.google.com/file/d/1BE-X58V7DTPoJwxPkfCyPNb1WISauwqK/view?usp=sharing>slides</a> |
        <a href=https://drive.google.com/file/d/1pS_OMZ83EG-oalD-30vDa3Ru49GWi-Ky/view?usp=sharing>poster</a> |
        <a href="https://www.youtube.com/watch?v=xB2gNx8f8Xc&feature=emb_title">oral video</a> |
        <a href="https://github.com/PruneTruong/GLU-Net">code</a>
        <p></p>
        We propose GLU-Net, a unified architecture to estimate dense correspondences between any image pair, i.e. different
        views of the same scene, consecutive frames of a video or even different instances of the same object category.
        <p></p>
      </td>
    </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- <tr onmouseout="wizard_stop()" onmouseover="wizard_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="dist_stop()" onmouseover="dist_start()">
      <td width="25%">
        <img src='media/thumnail_glampoints.png' width="200"></div>
      </td>
      <td valign="top" width="75%">
            <papertitle>GLAMpoints: Greedily Learned Accurate Match points</papertitle></a>
      <br>
          <strong>Prune Truong</strong>,
          Stefanos Apostolopoulos,
          Agata Mosinska,
          Samuel Stucky,
        Carlos Ciller,
        Sandro De Zanet
      <br>
          <em><strong>ICCV</strong> 2019</em>

      <br>
        <a href="bib/glampoints2019.txt">citation</a> |
        <a href="https://arxiv.org/abs/1908.06812">paper</a> |
        <a href=https://drive.google.com/file/d/1706DRoMCr7adBnxws1iLcwUfJ1ClEant/view?usp=sharing>poster</a> |
        <a href="https://github.com/PruneTruong/GLAMpoints_pytorch">code</a>
        <p></p>
        We propose a training strategy for keypoint detection, applicable to low-quality and textureless images,
        frequent in the medical domain. We learn keypoint detection by training directly for the final matching accuracy
        instead of indirect metrics such as repeatability.
        <p></p>
      </td>
    </tr>
      </table>

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Invited Talks</heading>
        </td>
      </tr>
  </table> -->

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;">
    <heading>Invited Talks</heading>
    <!-- <p></p> -->
    <br>
    <ul>
    <li><strong>2023:</strong> <strong>Dense Matching and Its Applications</strong>, Invited talk for the <a href="https://wasp-sweden.org/">Swedish WASP program</a>, Zurich.</li>
      <br>
      <li><strong>2022:</strong> <strong>Dense Matching</strong>, Invited talk at Google, Semantic Perception, Zurich.</li>
    <br>
      <li><strong>2022:</strong> <strong>Dense Matching</strong>, Invited talk at Microsoft, Mixed Reality and AI Lab, Zurich.</li>
    <br>
      <li><strong>2021:</strong> <strong>PDC-Net and matching challenge</strong>,  <a href=https://image-matching-workshop.github.io/> Image Matching Workshop: Local Features & Beyond</a>, CVPR 2021, Virtual.</li>
    <br>
      <li><strong>2021:</strong> <strong>PDC-Net (CVPR 2021)</strong>, Reading group of Dr. Krystian Mikolajczyk at <a href=https://www.imperial.ac.uk/matchlab>Matchlab</a>, Virtual.</li>
    <br>
      <li><strong>2021:</strong> <strong>GOCor (NeurIPS 2020)</strong>, <a href=https://www.youtube.com/watch?v=V-kPP9buPYU>Computer Vision Talks</a>, Virtual.</li>
    </div></div>
    </ul>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
          <!-- <br> -->
          <!-- Last updated: Mar 2023 -->
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
