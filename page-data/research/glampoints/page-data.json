{"componentChunkName":"component---src-templates-research-tsx","path":"/research/glampoints","result":{"data":{"post":{"html":"<h1><div align=\"center\">GLAMpoints: Greedily Learned Accurate Match points</div></h1>\n<h2><div align=\"center\">ICCV 2019</div></h2>\n<h2><div align=\"center\">Prune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky, Carlos Ciller, Sandro~De~Zanet</div></h2>\n<figure inline style=\"width: 100%\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/prun0/static/d378a547227b50b288e8093759c915dd/b59fb/method_2.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 29.166666666666668%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsSAAALEgHS3X78AAABp0lEQVQY03WQTYsSAQCG51I/IPoDQdClQy506l8UtbBBEAUdg4ouBSttpyD6FBQDIYoVW22rWWNM06lktWlFWMs2FcfxYx1Hp1wnnRR9dliCTr2X9/IenvcRJpMJs9mM6XRKr9ejv9Onrm2T3yii6watVpNms4lpmmiahmUNaKgd7i4GiYbXCdwXeflMJpfZ4tGtEAJ/02g0EMU10usyt68HOHbwEs+fvCYWE/nwMUWxWCS4vEy9USOdyHN43wKXF+5x/MAFzpy4wcOlIIeEUwjxeBxJkvYIIuEIieQ7XgRiXDn7gHRcYXU1RMrZKJ8VPB4PhUIBezgm+SbL140S78UcXz59Ry23kSJZBLfbjdfrpdvtEo1GkeUUpW8qybcKeqtHIiGRy2b2rhpdA9u2GVo2/4tQqVRot9uOL5051xyn50+ydM3Lkf3zPL7zFJfrKBfPnWf0e0i1WmVkj9DK28hrClubKrVSC/VHi02l7FAr/xx2Oh18Ph8rkRDSqww3r/rIKXn8fj/hlTB/xmMMw2DstGnsMOhb6A6IVqtj9n5iDSy6+i92AXVHi6gDAgPKAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"method 2\" title=\"method 2\" src=\"/prun0/static/d378a547227b50b288e8093759c915dd/d9199/method_2.png\" srcset=\"/prun0/static/d378a547227b50b288e8093759c915dd/8ff5a/method_2.png 240w,\n/prun0/static/d378a547227b50b288e8093759c915dd/e85cb/method_2.png 480w,\n/prun0/static/d378a547227b50b288e8093759c915dd/d9199/method_2.png 960w,\n/prun0/static/d378a547227b50b288e8093759c915dd/b59fb/method_2.png 1285w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n  <figcaption>a) Training steps for an image pair I_i and I&apos;_i at epoch $i$ created from a particular base image $B$. $I_i$ and $I&apos;_i$ are created by warping B according to homographies $g_i$ and $g&apos;_i$ respectively. $a_i$ and $a&apos;_i$ refer to the additional appearance augmentations applied to each image. b) Loss computation corresponding to situation a. c) Schematic representation of Unet-4</figcaption>\n</figure>\n<h2>Abstract</h2>\n<p>We introduce a novel CNN-based feature point detector - \\ac{GLAMpoints} - learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images. Our method can also be extended to other domains, such as natural images. Training code and model weights are available <a href=\"https://github.com/PruneTruong/GLAMpoints_pytorch\">here</a>.</p>\n<h2>Visual Results:</h2>\n<ul>\n<li>Aligning images of the ETH3D dataset</li>\n</ul>\n<p>Here, we warp the source images toward the target image. We compare our GLU-Net to state-of-the-art optical flow method PWC-Net and geometric matching method DGC-Net. </p>\n<div class=\"flex-row\">\n    <figure>\n      <img src=\"./ETH3D/playground_0_40_source.gif\">\n      <figcaption>Source images</figcaption>\n    </figure>\n    <figure>\n      <img src=\"./ETH3D/playground_0_40_target.gif\">\n      <figcaption>Target image</figcaption>\n    </figure>\n    <figure>\n      <img src=\"./ETH3D/playground_0_40_GLUNet.gif\">\n      <figcaption>**GLU-Net (Ours)**</figcaption>\n    </figure>\n    <figure>\n      <img src=\"./ETH3D/playground_0_40_PWCNet.gif\">\n      <figcaption>PWC-Net</figcaption>\n    </figure>\n    <figure>\n      <img src=\"./ETH3D/playground_0_40_DGCNet.gif\">\n      <figcaption>DGC-Net </figcaption>\n    </figure>\n</div>\n<h2>How to cite:</h2>\n<pre><code>@inproceedings{Truong2019GLAMpoints,\n          title={GLAMpoints: Greedily Learned Accurate Match Points},\n          author={Prune Truong and Stefanos Apostolopoulos and Agata Mosinska and Samuel Stucky and Carlos Ciller and Sandro De Zanet},\n          journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n          year={2019},\n          pages={10731-10740}\n}\n</code></pre>","frontmatter":{"date":"2019-08-01","path":"/research/glampoints","title":"GLAMpoints [ICCV 2019]","links":[{"type":"github","link":"https://github.com/PruneTruong/GLAMpoints_pytorch"},{"type":"arxiv","link":"https://arxiv.org/abs/1908.06812"}]},"timeToRead":1,"excerpt":"GLAMpoints: Greedily Learned Accurate Match points ICCV 2019 Prune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky, Carlosâ€¦"}},"pageContext":{}},"staticQueryHashes":["4181098705"]}